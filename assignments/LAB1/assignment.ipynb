{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLC6bgG7vhK3"
   },
   "source": [
    "# Lab 1: Algorithmic Decision Making 1\n",
    "\n",
    "The purpose of this assignment is for you to gain experience with basic methods for training, evaluating, and critiquing machine learning models\n",
    "using some of the most common Python machine learning libraries:\n",
    " - [Matplotlib](https://matplotlib.org/)\n",
    " - [NumPy](https://numpy.org/)\n",
    " - [Pandas](https://pandas.pydata.org/)\n",
    " - [scikit-learn](https://scikit-learn.org/stable/index.html)\n",
    "\n",
    "This assignment is intended to be done collaboratively.  You will work as a group to complete the coding and written portions of this assignment.  However, **each group member will submit a copy of the code** via the autograder and **the group will jointly submit one version of the written portion** of the assignment via Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Reading the Readme\n",
    "\n",
    "Before continuing with this notebook, first make sure to thoroughly read the `README.md` file at the base of this repository.\n",
    "The readme file contains useful information that you will use throughout all of your assignments.\n",
    "Refer back to this file whenever you have questions in this or future assignments.\n",
    "\n",
    "If you need more help with machine learning basics,\n",
    "we recommend seeing if there is a corresponding module in Google's [Introduction to Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3D7lMy3vhK5"
   },
   "source": [
    "## Part 1: Loading Data\n",
    "\n",
    "First, we will import modules that will let us do lots quickly.\n",
    "We'll import some powerful tools for importing and working with data, especially `pandas`.\n",
    "\n",
    "Pandas has been in development since 2008, largely through the efforts of one developer.\n",
    "For more on the history see [here](https://en.wikipedia.org/wiki/Pandas_(software).\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/) is a plotting library for Python built upon the concepts and semantics of [MATLAB](https://en.wikipedia.org/wiki/MATLAB),\n",
    "hence the name Matplotlib (\"MATlab PLOTting LIBrary\").\n",
    "Matplotlib is one of the most popular Python plotting libraries\n",
    "(with one of the others being [seaborn](https://seaborn.pydata.org/)).\n",
    "Although for simple plots (like histograms),\n",
    "you can sometimes get away with using [simpler methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html).\n",
    "\n",
    "[NumPy](https://numpy.org/) is a scientific computing package for Python that is ubiquitous in the data science / machine learning community.\n",
    "The most commonly used feature of NumPy is its [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)\n",
    "(N-dimensional array),\n",
    "which is an array that can exist in more than one dimension.\n",
    "In 1-dimension an ndarray is like a list,\n",
    "in 2-dimensions an ndarray is like a grid,\n",
    "in 3-dimensions an ndarray is like a cube,\n",
    "and in 4-dimensions and up an ndarray is like a hypercube.\n",
    "You can use the `shape` attribute to see the dimensions of an ndarray.\n",
    "\n",
    "Throughout this course, we will be using the [Pandas library](https://pandas.pydata.org/) to manipulate data.\n",
    "This library is very large and can be quite complex,\n",
    "so if you want additional information or practice\n",
    "we recommend the official [Pandas Tutorial](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html).\n",
    "Throughout this course, you will be heavily using [Pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)\n",
    "(a DataFrame is also frequently just called a \"frame\").\n",
    "\n",
    "scikit-learn is a large library that contains tools for classification, regression, clustering, feature selection, evaluation, and preprocessing.\n",
    "We will also refer to \"scikit-learn\" as \"sklearn\".\n",
    "Since Python doesn't like package names with a dash, `sklearn` is what you use to import the library in Python.\n",
    "It uses a fairly consistent API that lets you swap out components without needing the change your entire infrastructure,\n",
    "e.g., you can swap out the type of classifier you use without needing to change the rest of your code.\n",
    "It is so ubiquitous in machine learning that other machine learning libraries will often use the same terminology and methods (which we will discuss later).\n",
    "If you need instruction on scikit-learn, we recommend this [text tutorial](https://scikit-learn.org/stable/getting_started.html)\n",
    "and this [video tutorial](https://www.youtube.com/watch?v=0B5eIE_1vpU).\n",
    "sklearn also has a very complete [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "and many curated [examples](https://scikit-learn.org/stable/auto_examples/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5Ie4zRlvhK6"
   },
   "outputs": [],
   "source": [
    "# Import all libraries we will need.\n",
    "# It is good style to make all your imports in the first cell of the notebook.\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import pandas\n",
    "import sklearn.base\n",
    "import sklearn.datasets\n",
    "import sklearn.exceptions\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.tree\n",
    "\n",
    "# Set a global seed in case we need some randomness.\n",
    "random.seed(146)\n",
    "\n",
    "# We are going to ignore a specific warning from scikit-learn.\n",
    "# In real-world code you would confront this warning with better data munging and hyperparameter tweaking,\n",
    "# but those are not the focus of this course.\n",
    "warnings.simplefilter(\"ignore\", category = sklearn.exceptions.ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyShCQZCvhK-"
   },
   "source": [
    "The data we are using for this lab is from the [\"Communities and Crime\" dataset](https://archive.ics.uci.edu/dataset/183/communities+and+crime)\n",
    "available at [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/datasets).\n",
    "It includes data about the crime rates in various communities,\n",
    "including information about the socioeconomic, racial, and policing status in each community.\n",
    "\n",
    "We can easily load this data using the [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "error",
     "timestamp": 1599233556911,
     "user": {
      "displayName": "Andrew Thach",
      "photoUrl": "",
      "userId": "07507917617390649694"
     },
     "user_tz": 420
    },
    "id": "JMW4eiP-vhK_",
    "outputId": "10cdc80c-d779-472a-8d28-7f1e8ec9522f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1994 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0           0.19           0.33          0.02          0.90          0.12   \n",
       "1           0.00           0.16          0.12          0.74          0.45   \n",
       "2           0.00           0.42          0.49          0.56          0.17   \n",
       "3           0.04           0.77          1.00          0.08          0.12   \n",
       "4           0.01           0.55          0.02          0.95          0.09   \n",
       "...          ...            ...           ...           ...           ...   \n",
       "1989        0.01           0.40          0.10          0.87          0.12   \n",
       "1990        0.05           0.96          0.46          0.28          0.83   \n",
       "1991        0.16           0.37          0.25          0.69          0.04   \n",
       "1992        0.08           0.51          0.06          0.87          0.22   \n",
       "1993        0.20           0.78          0.14          0.46          0.24   \n",
       "\n",
       "      racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0            0.17         0.34         0.47         0.29        0.32  ...   \n",
       "1            0.07         0.26         0.59         0.35        0.27  ...   \n",
       "2            0.04         0.39         0.47         0.28        0.32  ...   \n",
       "3            0.10         0.51         0.50         0.34        0.21  ...   \n",
       "4            0.05         0.38         0.38         0.23        0.36  ...   \n",
       "...           ...          ...          ...          ...         ...  ...   \n",
       "1989         0.16         0.43         0.51         0.35        0.30  ...   \n",
       "1990         0.32         0.69         0.86         0.73        0.14  ...   \n",
       "1991         0.25         0.35         0.50         0.31        0.54  ...   \n",
       "1992         0.10         0.58         0.74         0.63        0.41  ...   \n",
       "1993         0.77         0.50         0.62         0.40        0.17  ...   \n",
       "\n",
       "      LandArea  PopDens  PctUsePubTrans  PolicCars  PolicOperBudg  \\\n",
       "0         0.12     0.26            0.20       0.06           0.04   \n",
       "1         0.02     0.12            0.45       0.00           0.00   \n",
       "2         0.01     0.21            0.02       0.00           0.00   \n",
       "3         0.02     0.39            0.28       0.00           0.00   \n",
       "4         0.04     0.09            0.02       0.00           0.00   \n",
       "...        ...      ...             ...        ...            ...   \n",
       "1989      0.01     0.28            0.05       0.00           0.00   \n",
       "1990      0.02     0.37            0.20       0.00           0.00   \n",
       "1991      0.08     0.32            0.18       0.08           0.06   \n",
       "1992      0.03     0.38            0.33       0.02           0.02   \n",
       "1993      0.11     0.30            0.05       0.08           0.04   \n",
       "\n",
       "      LemasPctPolicOnPatr  LemasGangUnitDeploy  LemasPctOfficDrugUn  \\\n",
       "0                    0.90                  0.5                 0.32   \n",
       "1                    0.00                  0.0                 0.00   \n",
       "2                    0.00                  0.0                 0.00   \n",
       "3                    0.00                  0.0                 0.00   \n",
       "4                    0.00                  0.0                 0.00   \n",
       "...                   ...                  ...                  ...   \n",
       "1989                 0.00                  0.0                 0.00   \n",
       "1990                 0.00                  0.0                 0.00   \n",
       "1991                 0.78                  0.0                 0.91   \n",
       "1992                 0.79                  0.0                 0.22   \n",
       "1993                 0.73                  0.5                 1.00   \n",
       "\n",
       "      PolicBudgPerPop  ViolentCrimesPerPop  \n",
       "0                0.14                  0.0  \n",
       "1                0.00                  1.0  \n",
       "2                0.00                  1.0  \n",
       "3                0.00                  0.0  \n",
       "4                0.00                  0.0  \n",
       "...               ...                  ...  \n",
       "1989             0.00                  0.0  \n",
       "1990             0.00                  1.0  \n",
       "1991             0.28                  0.0  \n",
       "1992             0.18                  0.0  \n",
       "1993             0.13                  1.0  \n",
       "\n",
       "[1994 rows x 123 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_data = pandas.read_csv(\"communities.csv\")\n",
    "\n",
    "# The last value in an iPython code cell will get output.\n",
    "communities_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see more specific stats about a DataFrame using the \n",
    "[DataFrame.info()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html)\n",
    "and [DataFrame.describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) methods.\n",
    "\n",
    "The [DataFrame.info()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method gives us high-level information about the frame itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1994 entries, 0 to 1993\n",
      "Columns: 123 entries, population to ViolentCrimesPerPop\n",
      "dtypes: float64(123)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "communities_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the [DataFrame.describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)\n",
    "gives us information all all **numeric** columns in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.057593</td>\n",
       "      <td>0.463395</td>\n",
       "      <td>0.179629</td>\n",
       "      <td>0.753716</td>\n",
       "      <td>0.153681</td>\n",
       "      <td>0.144022</td>\n",
       "      <td>0.424218</td>\n",
       "      <td>0.493867</td>\n",
       "      <td>0.336264</td>\n",
       "      <td>0.423164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>0.232854</td>\n",
       "      <td>0.161685</td>\n",
       "      <td>0.026093</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.111760</td>\n",
       "      <td>0.070461</td>\n",
       "      <td>0.094052</td>\n",
       "      <td>0.031209</td>\n",
       "      <td>0.292377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.126906</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.253442</td>\n",
       "      <td>0.244039</td>\n",
       "      <td>0.208877</td>\n",
       "      <td>0.232492</td>\n",
       "      <td>0.155196</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.166505</td>\n",
       "      <td>0.179185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109459</td>\n",
       "      <td>0.203092</td>\n",
       "      <td>0.229055</td>\n",
       "      <td>0.104581</td>\n",
       "      <td>0.062672</td>\n",
       "      <td>0.270038</td>\n",
       "      <td>0.228819</td>\n",
       "      <td>0.240328</td>\n",
       "      <td>0.097190</td>\n",
       "      <td>0.454969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "count  1994.000000    1994.000000   1994.000000   1994.000000   1994.000000   \n",
       "mean      0.057593       0.463395      0.179629      0.753716      0.153681   \n",
       "std       0.126906       0.163717      0.253442      0.244039      0.208877   \n",
       "min       0.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "25%       0.010000       0.350000      0.020000      0.630000      0.040000   \n",
       "50%       0.020000       0.440000      0.060000      0.850000      0.070000   \n",
       "75%       0.050000       0.540000      0.230000      0.940000      0.170000   \n",
       "max       1.000000       1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       racePctHisp  agePct12t21  agePct12t29  agePct16t24   agePct65up  ...  \\\n",
       "count  1994.000000  1994.000000  1994.000000  1994.000000  1994.000000  ...   \n",
       "mean      0.144022     0.424218     0.493867     0.336264     0.423164  ...   \n",
       "std       0.232492     0.155196     0.143564     0.166505     0.179185  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.010000     0.340000     0.410000     0.250000     0.300000  ...   \n",
       "50%       0.040000     0.400000     0.480000     0.290000     0.420000  ...   \n",
       "75%       0.160000     0.470000     0.540000     0.360000     0.530000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "          LandArea      PopDens  PctUsePubTrans    PolicCars  PolicOperBudg  \\\n",
       "count  1994.000000  1994.000000     1994.000000  1994.000000    1994.000000   \n",
       "mean      0.065231     0.232854        0.161685     0.026093       0.012272   \n",
       "std       0.109459     0.203092        0.229055     0.104581       0.062672   \n",
       "min       0.000000     0.000000        0.000000     0.000000       0.000000   \n",
       "25%       0.020000     0.100000        0.020000     0.000000       0.000000   \n",
       "50%       0.040000     0.170000        0.070000     0.000000       0.000000   \n",
       "75%       0.070000     0.280000        0.190000     0.000000       0.000000   \n",
       "max       1.000000     1.000000        1.000000     1.000000       1.000000   \n",
       "\n",
       "       LemasPctPolicOnPatr  LemasGangUnitDeploy  LemasPctOfficDrugUn  \\\n",
       "count          1994.000000          1994.000000          1994.000000   \n",
       "mean              0.111760             0.070461             0.094052   \n",
       "std               0.270038             0.228819             0.240328   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%               0.000000             0.000000             0.000000   \n",
       "50%               0.000000             0.000000             0.000000   \n",
       "75%               0.000000             0.000000             0.000000   \n",
       "max               1.000000             1.000000             1.000000   \n",
       "\n",
       "       PolicBudgPerPop  ViolentCrimesPerPop  \n",
       "count      1994.000000          1994.000000  \n",
       "mean          0.031209             0.292377  \n",
       "std           0.097190             0.454969  \n",
       "min           0.000000             0.000000  \n",
       "25%           0.000000             0.000000  \n",
       "50%           0.000000             0.000000  \n",
       "75%           0.000000             1.000000  \n",
       "max           1.000000             1.000000  \n",
       "\n",
       "[8 rows x 123 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green; font-size: x-large\";>♦ Group Break</h3>\n",
    "\n",
    "Take a moment to examine the data with your group.\n",
    "Read the [description of the dataset](https://archive.ics.uci.edu/dataset/183/communities+and+crime)\n",
    "and make sure you can match what you read on the webpage with what you see in the actual data.\n",
    "\n",
    "Some potential things to discuss with your group:\n",
    " - Are there any flaws in the collection methodology for this dataset?\n",
    " - Does this data contain any biasses?\n",
    " - How could this data be ethically used?\n",
    " - How could this data be unethically used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Features and Labels\n",
    "\n",
    "The first step when working with data (after loading it) is to define the features ($ X $) and labels ($ Y $).\n",
    "\n",
    "[Features](https://en.wikipedia.org/wiki/Feature_(machine_learning)) are the meaningful numbers that we associate with a data point.\n",
    "Our communities dataset has 123 columns, 122 of those are features.\n",
    "Some of the features in our dataset include `population`, `householdsize`, and `PolicCars`.\n",
    "Features are collectively referred to using the $ X $ variable,\n",
    "with specific instances represented with $ x $.\n",
    "Our machine learning models can have anywhere from a few to trillions of features.\n",
    "(Technically with the type of machine learning we will discuss in this class, models can have any finite number of features.)\n",
    "\n",
    "A label is the target of our predictive model.\n",
    "A label can be just about anything,\n",
    "but the most common types of labels are\n",
    "binary/boolean values (e.g., `true`/`false`, `yes`/`no`, `1`/`0`),\n",
    "members of a specified class (e.g., `dog` and `cat` can be valid labels for the `animal` class),\n",
    "or continuous values (often between 0.0 and 1.0).\n",
    "Features are collectively referred to using the $ Y $ variable,\n",
    "with specific instances represented with $ y $.\n",
    "\n",
    "In our dataset, the `ViolentCrimesPerPop` column is our label.\n",
    "If we take a look at this column, we can see that it is a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1994.000000\n",
       "mean        0.292377\n",
       "std         0.454969\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: ViolentCrimesPerPop, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_data['ViolentCrimesPerPop'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we look even closer,\n",
    "we can see that there are only two values here: 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_data['ViolentCrimesPerPop'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even through the column is continuous,\n",
    "the label is actually binary.\n",
    "Where 1.0 represents a high rate of violent crime in the community,\n",
    "and 0.0 represents a low rate of violent crime in the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we work more with our data, we are going to shuffle it.\n",
    "It is almost always a good idea to shuffle your data before working with it in machine learning to avoid seeing patterns that are not representative of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1994 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0           0.04           0.34          0.02          0.94          0.04   \n",
       "1           0.05           0.32          0.04          0.94          0.06   \n",
       "2           0.25           0.37          0.40          0.62          0.13   \n",
       "3           0.04           0.53          0.36          0.68          0.11   \n",
       "4           0.01           0.39          0.02          0.96          0.05   \n",
       "...          ...            ...           ...           ...           ...   \n",
       "1989        0.10           0.78          0.40          0.36          0.22   \n",
       "1990        0.00           0.44          0.00          0.99          0.03   \n",
       "1991        0.07           0.44          0.33          0.54          0.09   \n",
       "1992        0.05           0.54          0.74          0.40          0.05   \n",
       "1993        0.02           0.49          0.22          0.73          0.09   \n",
       "\n",
       "      racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0            0.01         0.49         0.55         0.39        0.44  ...   \n",
       "1            0.03         0.37         0.52         0.33        0.42  ...   \n",
       "2            0.05         0.57         0.66         0.55        0.46  ...   \n",
       "3            0.05         0.54         0.52         0.48        0.57  ...   \n",
       "4            0.02         0.53         0.69         0.66        0.35  ...   \n",
       "...           ...          ...          ...          ...         ...  ...   \n",
       "1989         0.58         0.50         0.53         0.31        0.19  ...   \n",
       "1990         0.00         0.40         0.47         0.29        0.28  ...   \n",
       "1991         0.37         0.49         0.69         0.53        0.29  ...   \n",
       "1992         0.01         0.61         0.68         0.57        0.39  ...   \n",
       "1993         0.25         0.39         0.51         0.33        0.44  ...   \n",
       "\n",
       "      LandArea  PopDens  PctUsePubTrans  PolicCars  PolicOperBudg  \\\n",
       "0         0.04     0.22            0.02       0.00            0.0   \n",
       "1         0.04     0.27            0.10       0.00            0.0   \n",
       "2         0.07     0.55            0.61       0.14            0.1   \n",
       "3         0.04     0.19            0.13       0.00            0.0   \n",
       "4         0.04     0.09            0.06       0.00            0.0   \n",
       "...        ...      ...             ...        ...            ...   \n",
       "1989      0.06     0.28            0.05       0.00            0.0   \n",
       "1990      0.02     0.15            0.01       0.00            0.0   \n",
       "1991      0.09     0.14            0.05       0.00            0.0   \n",
       "1992      0.07     0.15            0.01       0.00            0.0   \n",
       "1993      0.01     0.41            0.17       0.00            0.0   \n",
       "\n",
       "      LemasPctPolicOnPatr  LemasGangUnitDeploy  LemasPctOfficDrugUn  \\\n",
       "0                    0.00                  0.0                 0.00   \n",
       "1                    0.00                  0.0                 0.00   \n",
       "2                    0.45                  0.0                 0.78   \n",
       "3                    0.00                  0.0                 0.00   \n",
       "4                    0.00                  0.0                 0.00   \n",
       "...                   ...                  ...                  ...   \n",
       "1989                 0.00                  0.0                 0.00   \n",
       "1990                 0.00                  0.0                 0.00   \n",
       "1991                 0.00                  0.0                 0.00   \n",
       "1992                 0.00                  0.0                 0.00   \n",
       "1993                 0.00                  0.0                 0.00   \n",
       "\n",
       "      PolicBudgPerPop  ViolentCrimesPerPop  \n",
       "0                0.00                  0.0  \n",
       "1                0.00                  0.0  \n",
       "2                0.27                  1.0  \n",
       "3                0.00                  0.0  \n",
       "4                0.00                  0.0  \n",
       "...               ...                  ...  \n",
       "1989             0.00                  1.0  \n",
       "1990             0.00                  0.0  \n",
       "1991             0.00                  1.0  \n",
       "1992             0.00                  1.0  \n",
       "1993             0.00                  0.0  \n",
       "\n",
       "[1994 rows x 123 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_data = communities_data.sample(frac = 1, ignore_index = True, random_state = 146)\n",
    "communities_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should show up in a different order than it did before.\n",
    "Also note that we set the seed (`random_state`) for our shuffle.\n",
    "So technically, this notebook will always give the same order.\n",
    "That may seem to defeat the purpose of shuffling, but we do it for educational purposes.\n",
    "It would be really hard to write an assignment without knowing what the data looks like.\n",
    "In a real-world setting, you would set the random seed at the beginning of your process and print the seed just in case you need it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lRdwhZXvhLP"
   },
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 2.A</h3>\n",
    "\n",
    "Complete the function below that takes in a dataset as a Pandas DataFrame and the name of the label column,\n",
    "and return the features (as a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html))\n",
    "and labels (as a [Pandas Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)).\n",
    "The original DataFrame passed into the function should not be modified,\n",
    "and the returned features should not contain the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5GFkffsvhLP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features have 1994 rows and 122 columns.\n",
      "First 10 rows of features (only first 5 columns):\n",
      "   population  householdsize  racepctblack  racePctWhite  racePctAsian\n",
      "0        0.04           0.34          0.02          0.94          0.04\n",
      "1        0.05           0.32          0.04          0.94          0.06\n",
      "2        0.25           0.37          0.40          0.62          0.13\n",
      "3        0.04           0.53          0.36          0.68          0.11\n",
      "4        0.01           0.39          0.02          0.96          0.05\n",
      "5        0.13           0.55          0.05          0.78          0.14\n",
      "6        0.01           0.76          0.01          0.44          0.15\n",
      "7        0.03           0.42          0.47          0.62          0.01\n",
      "8        0.05           0.60          0.05          0.94          0.06\n",
      "9        0.02           0.35          0.11          0.90          0.03\n",
      "First 10 labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    1.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "5    0.0\n",
       "6    1.0\n",
       "7    0.0\n",
       "8    0.0\n",
       "9    1.0\n",
       "Name: ViolentCrimesPerPop, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_labels(frame, label_column_name):\n",
    "    \"\"\"\n",
    "    Take in a dataset as a Pandas DataFrame and the name of the label column,\n",
    "    and return the features (as a DataFrame) and labels (as a Series).\n",
    "    \n",
    "    Returns:\n",
    "        A copy of the passed in frame without the label column.\n",
    "        A copy of the label column as a Pandas Series.\n",
    "    \"\"\"\n",
    "    \n",
    "    # tmp_DataFrame = pandas.DataFrame(frame.loc[:, frame.columns != label_column_name]).copy()\n",
    "    tmp_DataFrame = ((pandas.DataFrame(frame)).copy()).drop(columns = [label_column_name])\n",
    "\n",
    "    tmp_Series = pandas.Series(frame[label_column_name])\n",
    "    \n",
    "    return tmp_DataFrame, tmp_Series\n",
    "    # return NotImplemented, NotImplemented\n",
    "\n",
    "communities_features, communities_labels = slice_labels(communities_data, 'ViolentCrimesPerPop')\n",
    "if (communities_features is NotImplemented):\n",
    "    print(\"Using fake data!\")\n",
    "    \n",
    "    # This notebook will use fake data until you implement the above function.\n",
    "    communities_features, communities_labels = sklearn.datasets.make_classification(\n",
    "            n_samples = 2000, n_features = len(communities_data.columns) - 1, random_state = 146)\n",
    "\n",
    "    # Convert the fake data from numpy to pandas.\n",
    "    communities_features = pandas.DataFrame(communities_features)\n",
    "    communities_labels = pandas.Series(communities_labels).astype(numpy.float64)\n",
    "\n",
    "print(\"Features have %d rows and %d columns.\" % (len(communities_features), len(communities_features.columns)))\n",
    "\n",
    "print(\"First 10 rows of features (only first 5 columns):\")\n",
    "print(communities_features[communities_features.columns[:5]].head(10))\n",
    "\n",
    "print(\"First 10 labels:\")\n",
    "communities_labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data as features and labels, it is time to **split** our data.\n",
    "Before we jump into the details of splitting data, let's imagine a situation that shows why we need to split up our data.\n",
    "Imagine we make a classifier that just remembers every single data point it has ever seen.\n",
    "So on `fit()`, all it does is store each data point along with its label.\n",
    "Then when it comes time to predict, the classifier will just look to see if it has seen the point before.\n",
    "If it has seen the point it returns the real label,\n",
    "and if it has not seen the point it returns a random label.\n",
    "\n",
    "If we train this classifier with `communities_features` like we have with the classifiers we have been working with earlier in this assignment,\n",
    "then this classifier will always score 100% (since it has seen every single point).\n",
    "But as soon as we ask the classifier to predict on new data,\n",
    "it will just give random predictions.\n",
    "So in testing it will do perfect, but in the real world it will perform poorly.\n",
    "\n",
    "In this case it is easy to see that our classifier will never work in the real world,\n",
    "but it is easy to [unintentionally make models that memorize data points](https://bair.berkeley.edu/blog/2019/08/13/memorization/)\n",
    "(especially if you are using neural networks).\n",
    "So to help avoid this, we split our data into multiple non-overlapping parts: [train and test](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets).\n",
    "The *train dataset* (also called \"train set\", \"train split\", or just \"train\") is the data that we will use to train/fit our models.\n",
    "We can also compute our evaluation metrics (e.g. accuracy) on our train split,\n",
    "but the resulting score is just used for debugging purposes.\n",
    "The *test dataset* (also called \"test set\", \"test split\", or just \"test\") is the data that we will officially compute our evaluation metrics on.\n",
    "The test set should never be used for any training purposes and should remain secret until your model is ready to be evaluated.\n",
    "It is considered cheating if information \"leaks\" between the two data splits.\n",
    "\n",
    "<center><img src=\"predictive_models.png\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>Comic courtesy of <a href='https://xkcd.com/2169/'>xkcd</a>.</center>\n",
    "<center style='font-size: small'>When you don't split your data properly, you may unknowingly leak information.</center>\n",
    "\n",
    "Sometimes, you may even need a third split of data.\n",
    "This third split is called the *validation dataset* (also called \"validation set\", \"validation split\", or just \"validation\").\n",
    "We won't be going into the details of what the validation split is used for,\n",
    "but (if it exists) it is generally the same size as the test split and used to train [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter).\n",
    "Hyperparameters are options that are at a higher level than normal model parameters\n",
    "(like the option of which machine learning model to use).\n",
    "\n",
    "There is no exact number or percent that decides how much of your data should be in train vs test.\n",
    "It just depends on your domain and how much data you have available.\n",
    "If there are no special circumstances, putting aside 10% - 25% of your data for testing typically works well.\n",
    "\n",
    "sklearn has a very simple function available that will split data for you:\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "If you have more complex data, splitting by hand should is fairly straightforward\n",
    "(just make sure you don't skip data points or double include them in both splits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVRA0kwnvhLS"
   },
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 2.B</h3>\n",
    "\n",
    "Complete the function below that splits the passed in dataset (created by `slice_labels()`) into training and test data with the specified size.\n",
    "You may assume that the specified size will always be smaller than the size of the dataset.\n",
    "The returned collections should be DataFrames for features and Series for labels (as in `slice_labels()`).\n",
    "\n",
    "Note: `split_data()` should not change the ordering of the features and labels (for testing purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEV4mIcpvhLS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Features: 1000\n",
      "Num Train Labels: 1000\n",
      "Num Test Features: 994\n",
      "Num Test Labels: 994\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "def split_data(features, labels, num_train):\n",
    "    \"\"\"\n",
    "    Split the incoming dataset into train and test portions that preserve the original orderings of features and labels.\n",
    "    \n",
    "    Returns:\n",
    "        X_train - The first num_train rows of X (features).\n",
    "        X_test - The remaining rows of X (features).\n",
    "        Y_train - The first num_train rows of Y (labels).\n",
    "        Y_test - The remaining rows of Y (labels).\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = features.iloc[: num_train, :]\n",
    "    X_test = features.iloc[num_train :, :]\n",
    "    Y_train = labels.iloc[: num_train]\n",
    "    Y_test = labels.iloc[num_train :]\n",
    "\n",
    "\n",
    "    # X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(features, labels, train_size = num_train)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "    # return NotImplemented, NotImplemented, NotImplemented, NotImplemented\n",
    "    \n",
    "communities_features_train, communities_features_test, communities_labels_train, communities_labels_test = split_data(communities_features, communities_labels, 1000)\n",
    "\n",
    "# Use dummy data if the above function is not yet implemented.\n",
    "if (communities_features_train is NotImplemented):\n",
    "    print(\"Using fake splits!\")\n",
    "    communities_features_train = communities_features\n",
    "    communities_features_test = communities_features\n",
    "    communities_labels_train = communities_labels\n",
    "    communities_labels_test = communities_labels\n",
    "\n",
    "print(\"Num Train Features: %d\" % (len(communities_features_train)))\n",
    "print(\"Num Train Labels: %d\" % (len(communities_labels_train)))\n",
    "print(\"Num Test Features: %d\" % (len(communities_features_test)))\n",
    "print(\"Num Test Labels: %d\" % (len(communities_labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green; font-size: x-large\";>♦ Group Break</h3>\n",
    "\n",
    "Take a moment to discuss the data and splitting process with your group.\n",
    "\n",
    "Some potential things to discuss with your group:\n",
    " - Why did we shuffle the data?\n",
    "     - Does shuffling affect our splits?\n",
    "     - Can we just shuffle after splitting?\n",
    " - Our label, `ViolentCrimesPerPop`, is a binary value that represents whether or not there is \"a high rate of violent crime in the community\".\n",
    "     - Where is the cutoff?\n",
    "     - Who determines what a \"high rate\" is?\n",
    "     - Could the choice of threshold for determining a \"high rate\" be made in an unethical way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFMuZppWvhLU"
   },
   "source": [
    "## Part 3: Training/Fitting a Model\n",
    "\n",
    "scikit-learn (`sklearn`) is probably the most popular machine learning library for Python.\n",
    "Therefore, it's the go-to choice for us to use in the course.\n",
    "Since the focus of this course is not on the technical practice of machine learning,\n",
    "we will not be going into details on every step in the implementation of a model\n",
    "(as those are covered in the recommended course requirements for this course).\n",
    "However, here are some scikit-learn resources if you need an introduction or refresher:\n",
    " - [scikit-learn Homepage](https://scikit-learn.org/stable/index.html)\n",
    " - [API Documentation](https://scikit-learn.org/stable/modules/classes.html)\n",
    " - [Official User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    " - [Official Examples](https://scikit-learn.org/stable/auto_examples/index.html)\n",
    " - [Recommended Text Tutorial](https://scikit-learn.org/stable/getting_started.html)\n",
    " - [Recommended Video Tutorial](https://www.youtube.com/watch?v=0B5eIE_1vpU)\n",
    "\n",
    "Training is one of the core parts of the machine learning pipeline and is when we adjust our machine learning model based on our training data.\n",
    "You can say that this is the actual \"learning\" part of machine learning.\n",
    "(\"Learning\" is in quotes here because machine learning is **VERY** different from the how humans learn and pretty much any biological or physiological concept of learning.\n",
    "A better name for machine learning is probably something like \"computerized pattern recognition\" (*this name is only Eriq's opinion*).)\n",
    "In training (also called \"fitting\"), we give our machine learning model access to our training set so it can find patterns in the data.\n",
    "Once our model is trained/fitted, then it should be able to make predictions on data it has not seen before.\n",
    "\n",
    "In this assignment, we will be using two orthogonal machine learning algorithms:\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "and [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning).\n",
    "Although both are supervised machine learning techniques,\n",
    "they learn patterns in the data in very different ways.\n",
    "Logistic Regression fits a single continuous function to the data and uses training to tweak the parameters of that function.\n",
    "Whereas Decision Trees builds a structure called a \"decision tree\" so sort the data into different bins.\n",
    "(Note that here we are collectively discussing a family of techniques for generating decision trees,\n",
    "but in practice we will need to choose a specific algorithm (or let scikit-learn choose one for us).)\n",
    "\n",
    "Before we train our own models, we are going to define a dummy model.\n",
    "This will only be used in case you have not yet implemented different parts of this assignment.\n",
    "You can totally ignore this (or look at it, your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can ignore this, it is just a dummy classifier/model.\n",
    "class DummyClassifier(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, choices = [0, 1]):\n",
    "        self._choices = choices.copy()\n",
    "\n",
    "    def fit(self, train_features, train_labels):\n",
    "        pass\n",
    "\n",
    "    def predict(self, test_features):\n",
    "        return [random.choice(self._choices) for _ in range(len(test_features))]\n",
    "        \n",
    "    def score(self, test_features, test_labels):\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 3.A</h3>\n",
    "\n",
    "Complete the function below which takes in a training dataset, and returns a **trained** Logistic Regression model.\n",
    "The returned model should be a [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "You should use all the default constructor options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 train predictions:  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "First 10 test predictions:   [0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def get_trained_lr(features, labels):\n",
    "    \"\"\"\n",
    "    Take in a training dataset, and return a Logistic Regression model trained on that dataset.\n",
    "    \n",
    "    Returns:\n",
    "        A trained Logistic Regression model (sklearn.linear_model.LogisticRegression).\n",
    "    \"\"\"\n",
    "    lr_model = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "    return lr_model.fit(features, labels)\n",
    "\n",
    "    # return NotImplemented\n",
    "\n",
    "trained_lr = get_trained_lr(communities_features_train, communities_labels_train)\n",
    "\n",
    "# Use a dummy model if the above function is not yet implemented.\n",
    "if (trained_lr is NotImplemented):\n",
    "    print(\"Using fake model!\")\n",
    "    trained_lr = DummyClassifier()\n",
    "\n",
    "train_predictions = trained_lr.predict(communities_features_train)\n",
    "test_predictions = trained_lr.predict(communities_features_test)\n",
    "\n",
    "print(\"First 10 train predictions: \", train_predictions[:10])\n",
    "print(\"First 10 test predictions:  \", test_predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "Look at those predictions.\n",
    "\n",
    "Now do the same thing using Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 3.B</h3>\n",
    "\n",
    "Complete the function below which takes in a training dataset and optional max depth,\n",
    "and returns a **trained** Decision Tree model.\n",
    "The returned model should be a [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "You should use all the default constructor options,\n",
    "except for `max_depth` which should be taken in as an optional parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 train predictions:  [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "First 10 test predictions:   [0. 1. 0. 1. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def get_trained_dt(features, labels, max_depth = 5):\n",
    "    \"\"\"\n",
    "    Take in a training dataset and max depth, and return a Decision Tree model trained on that dataset.\n",
    "    \n",
    "    Returns:\n",
    "        A trained Decition Tree model (sklearn.tree.DecisionTreeClassifier).\n",
    "    \"\"\"\n",
    "\n",
    "    dt_model = sklearn.tree.DecisionTreeClassifier(max_depth = max_depth)\n",
    "\n",
    "    return dt_model.fit(features, labels)\n",
    "\n",
    "    # return NotImplemented\n",
    "\n",
    "trained_dt = get_trained_dt(communities_features_train, communities_labels_train)\n",
    "\n",
    "# Use a dummy model if the above function is not yet implemented.\n",
    "if (trained_dt is NotImplemented):\n",
    "    print(\"Using fake model!\")\n",
    "    trained_dt = DummyClassifier()\n",
    "\n",
    "train_predictions = trained_dt.predict(communities_features_train)\n",
    "test_predictions = trained_dt.predict(communities_features_test)\n",
    "\n",
    "print(\"First 10 train predictions: \", train_predictions[:10])\n",
    "print(\"First 10 test predictions:  \", test_predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have models that can make predictions, we will want to see how our models are doing.\n",
    "To evaluate our model's performance, we can use [evaluation metrics](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers#Single_metrics),\n",
    "which are numbers that quantify predictive performance (how well our predictions match reality).\n",
    "There are [dozens of evaluation metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) available in scikit-learn,\n",
    "and hundreds of recognized useful metrics.\n",
    "But for this lab, we are going to keep it simple and use mean accuracy available via our models' `score()` method.\n",
    "\n",
    "In real-world situations,\n",
    "deciding on an evaluation metric is one of the most important decisions made for a dataset/problem.\n",
    "Choosing the correct evaluation metric impacts many downstream decisions and can determine if your models are ultimately successful when deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score:  0.841\n",
      "Decision Trees Score:       0.807\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Score: \", trained_lr.score(communities_features_test, communities_labels_test))\n",
    "print(\"Decision Trees Score:      \", trained_dt.score(communities_features_test, communities_labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green; font-size: x-large\";>♦ Group Break</h3>\n",
    "\n",
    "Take a moment to discuss the training/fitting process with your group.\n",
    "\n",
    "Some potential things to discuss with your group:\n",
    " - Why do we need more than one supervised machine learning algorithm?\n",
    " - Are there any traits of a dataset that would make Logistic Regression better than Decision Trees, or visa versa?\n",
    " - What if my decision tree is infinitely big (or at least had as many leafs as training data points)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tqd00jXqvhLf"
   },
   "source": [
    "## Part 4: Visualizing Trade-Offs\n",
    "\n",
    "One of the most important things in machine learning is analyzing trade-offs.\n",
    "There will be countless decisions you have to make about your data, model, evaluation, visualization, and other things.\n",
    "Being able to conceptualize (and hopefully even visualize) these trade-offs is critical to making good models.\n",
    "In this section, we will explore just a few of these trade-offs and how we can visualize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89Iuyv30vhLi"
   },
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 4.A</h3>\n",
    "\n",
    "In this task, we will explore the trade-off of training set size.\n",
    "Data is one of our most valuable resources, and it can be costly to acquire (and label).\n",
    "So, we are constantly trying to see how our models will perform when only given small amounts of data.\n",
    "\n",
    "Complete the following function which will train a Logistic Regression and Decision Tree model on the passed in dataset with each of the specified training sizes.\n",
    "This function should return two lists that contain the accuracy (via the `score()` method) of each model for each training set size.\n",
    "Ensure that you use the test (not train) data to score each model.\n",
    "\n",
    "Make sure to use the functions that you have already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_train_size_scores(features, labels, train_sizes):\n",
    "    \"\"\"\n",
    "    Create and train models for each of the specified training set sizes,\n",
    "    and return the resulting scores for Logistic Regression and Decision Trees.\n",
    "\n",
    "    Returns:\n",
    "        A list of scores (one for each training set size) for a trained Logistic Regression model.\n",
    "        A list of scores (one for each training set size) for a trained Decision Trees model.\n",
    "    \"\"\"\n",
    "\n",
    "    lr_score = list()\n",
    "    dt_score = list() \n",
    "    for tmp_size in (train_sizes):\n",
    "\n",
    "        X_train, x_test, Y_train, y_test = split_data(features, labels, tmp_size)\n",
    "\n",
    "        lr_model = get_trained_lr(X_train, Y_train)\n",
    "        dt_model = get_trained_dt(X_train, Y_train)\n",
    "\n",
    "        lr_score.append(lr_model.score(x_test, y_test))\n",
    "        dt_score.append(dt_model.score(x_test, y_test))\n",
    "\n",
    "\n",
    "    return lr_score, dt_score\n",
    "    # return NotImplemented, NotImplemented\n",
    "\n",
    "train_sizes = list(range(10, 1010, 10))\n",
    "lr_scores, dt_scores = get_train_size_scores(communities_features, communities_labels, train_sizes)\n",
    "\n",
    "if (lr_scores is NotImplemented):\n",
    "    print(\"Function not implemented, using fake data!\")\n",
    "    lr_scores = [random.random() for _ in range(len(train_sizes))]\n",
    "    dt_scores = [random.random() for _ in range(len(train_sizes))]\n",
    "\n",
    "# Plot the results.\n",
    "matplotlib.pyplot.title('Training Set Size vs Score')\n",
    "matplotlib.pyplot.xlabel('Training Set Size')\n",
    "matplotlib.pyplot.ylabel('Model Score')\n",
    "\n",
    "matplotlib.pyplot.plot(train_sizes, lr_scores, color = 'red', label = 'Logistic Regression')\n",
    "matplotlib.pyplot.plot(train_sizes, dt_scores, color = 'blue', label = 'Decision Trees', linestyle = 'dashed')\n",
    "matplotlib.pyplot.legend()\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should be able to see that the scores for each model starts pretty low,\n",
    "but then increases as more training data is added.\n",
    "At some point we see that the model stops improving and the score more-or-less stays the same.\n",
    "\n",
    "Note that the amount of data we have is relatively low by machine learning standards,\n",
    "so that is why our plot is a bit bumpy.\n",
    "The more data you get, the more precise view you can get on your performance.\n",
    "\n",
    "Also note that in this specific problem, we only needed a few hundred points in our training set.\n",
    "For other machine learning problems, it is not unusual to require orders of magnitude more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 4.B</h3>\n",
    "\n",
    "In this task, we will explore the trade-off of model complexity.\n",
    "Part of the process for picking what machine learning model/algorithm to use also involves deciding how \"complex\" to make our model.\n",
    "Complexity can be a difficulty topic for beginners to quantify, but we can roughly think about complex models as having a lot of parameters/components.\n",
    "For example, we can say that a very \"simple\" decision tree for our data has one non-leaf (decision) node with two children: 1 and 0.\n",
    "Whereas a \"complex\" decision tree and have hundreds of decision nodes with dozens of leafs.\n",
    "Complex models can be nice, because they can capture natural complexities and nuances in our data.\n",
    "However, complex models can also be bad because instead of learning general patterns then may learn patterns that are specific to our training data\n",
    "(and not all general data).\n",
    "\n",
    "Complete the following function which will train a Decision Tree model on the passed in dataset with each of the specified max depths.\n",
    "\n",
    "This function should return two lists that contain the accuracy (via the `score()` method) of each model for each training set size.\n",
    "Ensure that you use the test (not train) data to score each model.\n",
    "\n",
    "This function should return two lists that contain the accuracy (via the `score()` method) of each instance of the Decision Tree model with each corresponding max depth.\n",
    "The **first list** should contain the result of scoring the model on **training data**.\n",
    "The **second list** should contain the result of scoring the model on **test data**.\n",
    "(If you are new to ML, the first list may seem unusual.\n",
    "Why score a model on data it was just trained on?\n",
    "This will hopefully make more sense when you see the resulting data plotted.)\n",
    "\n",
    "Make sure to use the functions that you have already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt_max_depth_scores(features, labels, max_depths, train_size = 1000):\n",
    "    \"\"\"\n",
    "    Create and train Decision Tree models for each of the specified max depths,\n",
    "    and return the resulting scores (on train and test data).\n",
    "\n",
    "    Returns:\n",
    "        A list of scores (one for each max depth) for a trained Decision Trees model, scored on the training data.\n",
    "        A list of scores (one for each max depth) for a trained Decision Trees model, scored on the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    train_scores = list()\n",
    "    test_scores = list()\n",
    "    for tmp_depth in max_depths:\n",
    "        X_train, x_test, Y_train, y_test = split_data(features, labels, train_size)\n",
    "\n",
    "        dt_model = get_trained_dt(X_train, Y_train, tmp_depth)\n",
    "\n",
    "        train_scores.append(dt_model.score(X_train, Y_train))\n",
    "        test_scores.append(dt_model.score(x_test, y_test))\n",
    "\n",
    "    return train_scores, test_scores\n",
    "    # return NotImplemented, NotImplemented\n",
    "\n",
    "max_depths = list(range(1, 21))\n",
    "train_scores, test_scores = get_dt_max_depth_scores(communities_features, communities_labels, max_depths)\n",
    "\n",
    "if (train_scores is NotImplemented):\n",
    "    print(\"Function not implemented, using fake data!\")\n",
    "    train_scores = [random.random() for _ in range(len(max_depths))]\n",
    "    test_scores = [random.random() for _ in range(len(max_depths))]\n",
    "\n",
    "# Plot the results.\n",
    "matplotlib.pyplot.title('Max Depth vs Score')\n",
    "matplotlib.pyplot.xlabel('Max Depth')\n",
    "matplotlib.pyplot.ylabel('Model Score')\n",
    "\n",
    "matplotlib.pyplot.plot(max_depths, train_scores, color = 'green', label = 'Training Score')\n",
    "matplotlib.pyplot.plot(max_depths, test_scores, color = 'blue', label = 'Test Score', linestyle = 'dashed')\n",
    "matplotlib.pyplot.legend()\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a few very interesting things going on in this graph.\n",
    "If we look at the test score (which we looked at in Task 4.A),\n",
    "we can see that the score starts lower, reaches its peak, but then starts to actually get worse.\n",
    "So our model actually gets worse if we make it too complex.\n",
    "\n",
    "To see why out model gets worse, we can look at the green line: the score on the training data.\n",
    "Looking at this line, we can see that it just increases as the model becomes more complex until it plateaus off at 100%.\n",
    "What we are actually looking at is the model becoming so complex that it learns all the tiny details that are present in the training set,\n",
    "but that are not specific to all the data.\n",
    "Like if we wanted to draw conclusions about all undergraduate students just by looking at the students in this course.\n",
    "Because the sample is not necessarily representative of all undergraduate students,\n",
    "if we get too complex we may learn patterns that do not hold for all undergraduate students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green; font-size: x-large\";>♦ Group Break</h3>\n",
    "\n",
    "Take a moment to discuss trade-offs made in machine learning.\n",
    "\n",
    "Some potential things to discuss with your group:\n",
    " - Can you always choose the correct point for a trade-off (e.g., can you always choose the correct number of data points to use for training)?\n",
    " - In Task 4.A, is there anything we could do to smooth out our plot with the data we have?\n",
    "     - Here we are not looking just to visually smooth our our plot by taking a different number of samples, but to actually improve the quality of our results.\n",
    " - Is it always bad for training error to go to zero (or training accuracy to go to 100%)?\n",
    " - What would it take to get a representative sample of undergraduate students?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPFIHnytvhLm"
   },
   "source": [
    "## Part 5: Observing Error Disparities\n",
    "\n",
    "We we train our model on some dataset,\n",
    "we say that the trained model has some error associated with it.\n",
    "So far in this lab, we have been using accuracy (from the `score()` method),\n",
    "but we can convert this into an error by subtracting that from $ 1.0 $.\n",
    "For example, a score of $ 0.75 $ is an error of $ 1.0 - 0.75 = 0.25 $.\n",
    "When doing more formal work we may use a more robust error metric,\n",
    "but there will always be a link between a score you are trying to maximize (e.g., we want a high accuracy)\n",
    "and an error/loss that you want to minimize.\n",
    "\n",
    "Sometimes it can be useful to see if our model is performing the same on different populations,\n",
    "or groups in our dataset.\n",
    "We can compute the disparity between these two groups by taking the absolute value of the difference in error between the same model tested on the different groups.\n",
    "For example if we had an image classifier,\n",
    "it could be useful to see if our model performs equally well for dogs and cats.\n",
    "If we see a disparity, then that may tell us we need more training images of the respective animal.\n",
    "\n",
    "Let's start looking for disparities in our data by creating the groups that we will look for disparities between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 5.A</h3>\n",
    "\n",
    "Complete the following function which will take in a DataFrame, column name, and threshold.\n",
    "The function will then return the data split into two groups based on the value in the specified column in relation to the threshold.\n",
    "\n",
    "The passed in DataFrame should not be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_column(frame, column_name, threshold):\n",
    "    \"\"\"\n",
    "    Take in a DataFrame, column name, and threshold;\n",
    "    and return two DataFrames that are split based on the specified column's value.\n",
    "    \n",
    "    Returns:\n",
    "        A Pandas DataFrame that contains all rows where the specified column are less than the threshold.\n",
    "        A Pandas DataFrame that contains all rows where the specified column are greater than or equal to the threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    return NotImplemented, NotImplemented\n",
    "\n",
    "under_group, over_group = split_on_column(communities_data, 'population', 0.05)\n",
    "\n",
    "if (under_group is NotImplemented):\n",
    "    print(\"Function not implemented.\")\n",
    "else:\n",
    "    print(\"Size of under group: '%d'.\" % (len(under_group)))\n",
    "    print(\"Size of over group: '%d'.\" % (len(over_group)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can split data based on a column and threshold,\n",
    "we can start to dive deep into our data and look for disparities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 5.B</h3>\n",
    "\n",
    "Complete the following function which will take in a DataFrame, column name, and label column name;\n",
    "and compute the score disparity (using the `score()` method) between the two groups.\n",
    "\n",
    "Include all the following notes when implementing your function:\n",
    " - When splitting **each** group into train and test sets, use a 50/50 split (half the data is train and half is test).\n",
    " - Use the mean value of the column as the threshold for splitting into groups.\n",
    "     - The [Pandas Series.mean()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mean.html) method should be very useful here.\n",
    " - Use your standard Logistic Regression classifier (`get_trained_lr()`).\n",
    " - You can assume that your groups will always contain multiple labels.\n",
    "     - It is possible the splitting a group may result in one group only have one label represented.\n",
    "     - scikit-learn will raise an exception in this case, just let that exception be raised.\n",
    " - Make sure to use the functions that you have already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(frame, column_name, label_column_name):\n",
    "    \"\"\"\n",
    "    Compute the disparity for a specific column in a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        The disparity for the specified column.\n",
    "    \"\"\"\n",
    "\n",
    "    return NotImplemented\n",
    "\n",
    "disparity = compute_disparity(communities_data, 'population', 'ViolentCrimesPerPop')\n",
    "if (disparity is NotImplemented):\n",
    "    print(\"Function not implemented.\")\n",
    "else:\n",
    "    print(\"Disparity on the population column: %0.4f.\" % (disparity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can compute the disparity for one column, we can compute it for all columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the column names.\n",
    "feature_names = list(communities_features.columns)\n",
    "disparities = []\n",
    "\n",
    "# Compute the disparity for each column.\n",
    "for feature_name in feature_names:\n",
    "    try:\n",
    "        disparity = compute_disparity(communities_data, feature_name, 'ViolentCrimesPerPop')\n",
    "    except:\n",
    "        disparity = numpy.nan\n",
    "        \n",
    "    if (disparity is NotImplemented):\n",
    "        disparity = numpy.nan\n",
    "        \n",
    "    disparities.append(disparity)\n",
    "\n",
    "# Merge the names and disparities into one list and sort by disparity (descending).\n",
    "sorted_pairs = sorted(zip(disparities, feature_names), reverse = True)\n",
    "    \n",
    "print(\"Disparities:\")\n",
    "for (disparity, feature_name) in sorted_pairs:\n",
    "    print(\"    %s -- %0.4f\" % (feature_name, disparity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see all the columns in our dataset ordered by disparities.\n",
    "Note that we don't know which direction the disparity goes: does it favor the under group or over group?\n",
    "All we know is that a high disparity means that the two groups are not being treated the same.\n",
    "Concerningly, we see a lot of high disparities on racial attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGwU5_u8vhLq"
   },
   "source": [
    "### Error Rate Disparities\n",
    "\n",
    "Instead of error disparities, let's compute two other types of errors that are of interest to us:\n",
    "False Negative Disparity (also called False Negative Error Rate Balance)\n",
    "and False Positive Disparity (also called False Positive Error Rate Balance).\n",
    "Both of these disparities fall under the category of [Fairness Metrics](https://en.wikipedia.org/wiki/Fairness_(machine_learning)#Definitions_based_on_predicted_and_actual_outcomes).\n",
    "These metrics will tell how about the gap in errors between two groups.\n",
    "Essentially, how more likely are we to make types of mistakes for different groups of people.\n",
    "\n",
    "To compute these metrics, we first need to be able to categorize each of our predictions outcomes.\n",
    "To do this, we can use a tool called a [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) or \"contingency table\".\n",
    "Confusion matrices allow us to categorize the results from a binary classifier into four different buckets that are more specific than just \"right\" and \"wrong\".\n",
    "\n",
    "<center><img src=\"confusion-matrix.png\"/></center>\n",
    "<center style='font-size: small'>Image courtesy of <a href='https://en.wikipedia.org/wiki/Confusion_matrix'>Wikipedia</a> 2024-03-20.</center>\n",
    "\n",
    " - True Positives (TP) -- Also called a \"hit\". Represent instances that were correctly classified as positive/true.\n",
    " - True Negatives (TN) -- Also called a \"correct rejection\". Represent instances that were correctly classified as negative/false.\n",
    " - False Positives (FP) -- Also called a \"Type I Error\" or \"false alarm\". Represent instances that were incorrectly classified as positive/true.\n",
    " - False Negatives (FN) -- Also called a \"Type II Error\" or \"miss\". Represent instances that were incorrectly classified as negative/false.\n",
    "\n",
    "Using these four values, we can compute all sorts of [useful statistics](https://en.wikipedia.org/wiki/Binary_classification#The_eight_basic_ratios),\n",
    "including the False Negative Rate (FNR) and False Positive Rate (FPR) that we need for the above disparities.\n",
    "\n",
    "| Metric              | Short Name | Definition                 |\n",
    "| ------------------- | ---------- | -------------------------- |\n",
    "| False Negative Rate | FNR        | $$ \\frac{FN}{(TP + FN)} $$ |\n",
    "| False Positive Rate | FPR        | $$ \\frac{FP}{(TN + FP)} $$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 5.C</h3>\n",
    "\n",
    "Complete the following function which will take in a DataFrame, column name, and label column name;\n",
    "and compute the FNR and FPR disparities between the two groups.\n",
    "This function has all the same semantics as `compute_disparity()`,\n",
    "but returns two values based on FNR and FPR (respectively) instead of `score()`.\n",
    "\n",
    "If any FNR/FPR computation will return in a division by zero,\n",
    "use `nunmpy.nan` instead.\n",
    "\n",
    "You may find the [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_disparity(frame, column_name, label_column_name):\n",
    "    \"\"\"\n",
    "    Compute the FNR and FPR disparities for a specific column in a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        The FNR disparity for the specified column.\n",
    "        The FPR disparity for the specified column.\n",
    "    \"\"\"\n",
    "\n",
    "    return NotImplemented, NotImplemented\n",
    "\n",
    "fnr_disparity, fpr_disparity = compute_confusion_disparity(communities_data, 'population', 'ViolentCrimesPerPop')\n",
    "if (fnr_disparity is NotImplemented):\n",
    "    print(\"Function not implemented.\")\n",
    "else:\n",
    "    print(\"FNR disparity on the population column: %0.4f.\" % (fnr_disparity))\n",
    "    print(\"FPR disparity on the population column: %0.4f.\" % (fpr_disparity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these additional metrics, we can take a closer look at some of the concerning columns that have high disparities in our data.\n",
    "Three of the four racial columns had high disparities, so let's take a look at those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all race* columns.\n",
    "column_names = sorted([column_name for column_name in communities_features.columns if str(column_name).startswith('race')])\n",
    "\n",
    "headers = [\n",
    "    \"column\",\n",
    "    \"Score Disparity\",\n",
    "    \"FNR Disparity\",\n",
    "    \"FPR Disparity\",\n",
    "]\n",
    "\n",
    "print(\"\\t\".join(headers))\n",
    "\n",
    "# Compute the disparities for each column.\n",
    "for column_name in column_names:\n",
    "    score_disparity = compute_disparity(communities_data, column_name, 'ViolentCrimesPerPop')\n",
    "    fnr_disparity, fpr_disparity = compute_confusion_disparity(communities_data, column_name, 'ViolentCrimesPerPop')\n",
    "    \n",
    "    if ((score_disparity is NotImplemented) or (fnr_disparity is NotImplemented)):\n",
    "        print(\"Functions not implemented.\")\n",
    "        continue\n",
    "        \n",
    "    row = [column_name] + [\"%0.4f\" % (value) for value in [score_disparity, fnr_disparity, fpr_disparity]]\n",
    "    print(\"\\t\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of interesting things going on in these metrics that you will discuss with your group and class.\n",
    "But for now, just look at the largest number (assuming your code is implemented correctly):\n",
    "FNR Disparity for the \"racePctWhite\" column at 0.70.\n",
    "Not only is this a huge disparity, but it reveals a troubling pattern in our data.\n",
    "Areas with predominantly white residents are more likely to be **incorrectly** given the negative class (\"low violent crime\")\n",
    "than areas that do not have predominantly white residents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green; font-size: x-large\";>♦ Group Break</h3>\n",
    "\n",
    "Take a moment to discuss some of these metrics you have computed in this data,\n",
    "and their ethical ramifications.\n",
    "\n",
    "Some potential things to discuss with your group:\n",
    " - Does any of this actually matter?\n",
    "     - So what if some stats are biased, does this actually affect people's lives?\n",
    "     - In what ways can these skewed statistics impact people's lives?\n",
    " - Are the different type of classification errors (FP and FN) equivalent (equally bad)?\n",
    "     - Are they equally harmful?\n",
    " - Why is there disparity in these racial statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TyI1c1uvhLu"
   },
   "source": [
    "## Part 6: Short Response Questions\n",
    "\n",
    "Please go to Canvas to enter your group's responses to the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1DZt_wYvhLu"
   },
   "source": [
    "#### Q1\n",
    "\n",
    "When training a machine learning model with some dataset, what are some assumptions we are making about the data?\n",
    "What are some things that it is important for us not to assume? Please give a few examples for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pE41hbB5vhLv"
   },
   "source": [
    "#### Q2\n",
    "\n",
    "Why is it important to evaluate our model on data which was not used in training?\n",
    "Why are there different error rates for train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYyxHEH2vhLw"
   },
   "source": [
    "#### Q3\n",
    "\n",
    "In your own words, explain the results of your plot from Task 4.A.\n",
    "Why does it make sense that these results occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERGdRbWYvhLw"
   },
   "source": [
    "#### Q4\n",
    "\n",
    "In your own words, explain the results of your plot from Task 4.B.\n",
    "Why does it make sense that these results occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W31Qzr74vhLx"
   },
   "source": [
    "#### Q5\n",
    "\n",
    "In your own words, explain what the list of sorted disparities (after Task 5.B) means.\n",
    "What are some possible implications of this model in terms of unfairness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UChV9F0vhLz"
   },
   "source": [
    "#### Q6\n",
    "\n",
    "Look through the available features on the [dataset we used](https://archive.ics.uci.edu/dataset/183/communities+and+crime).\n",
    "What are two attributes that you would expect to have high error disparity?\n",
    "What are two attributes you would expect to have low error disparity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-54DAPpwvhL0"
   },
   "source": [
    "#### Extra Credit\n",
    "\n",
    "Play around with the data and generate some kind of plot that you find interesting.\n",
    "Write a few sentences about your process, what you found, and what you think it suggests about the data.\n",
    "This could be an evaluation of multiple model classes,\n",
    "a statistical analysis of different features,\n",
    "unsupervised analysis,\n",
    "extending the investigation into error discrepancies,\n",
    "or anything else you can think of.\n",
    "\n",
    "Include an image of the plot, a description, and an analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
